{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4402e88f-7c29-4a10-9a3a-3376ffeeecc0",
   "metadata": {},
   "source": [
    "The first task that I did for this project is starting the tutorials and trying to piece what each part means. There were a few things that confused me at first so it took some time to understand how everything works together, especially since this was my first time using a template from scratch and using it with the other document and csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c52cab0-a05b-4cb4-ae52-89683236f545",
   "metadata": {},
   "source": [
    "Next, I used this practice and started to constuct my own website. I followed the second tutorial along with this. I started with the stuff I knew which was basic things like adding colors and changing text, which made me feel more organized in the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86600a02-390d-4eb7-aef6-22f3a769c2af",
   "metadata": {},
   "source": [
    "The website was blocked from scraping, which was the first time that I ahve encountered this. It through me off as I did not know how I was going to get the information. I tried (as a placement) to use what they published online to get things to work and then figure out a way to scrape, but these csv's would not work so I had to go back to what I had from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee13b1c8-f4ad-4398-a81c-54fe7e63dc96",
   "metadata": {},
   "source": [
    "This project helped me narrow down what scraping is, which was confusing to me for several weeks because it was the first type of coding that used multiple pieces that needed to work together in order to get something that would take too much time doing another way. Scraping is very important so this project did encourage me and also help me understand it. I did reach out to several professors during the process to make sure that I was doing the right thing before I scraped fully so I would not get blocked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1391f6ef-8922-4f48-91b1-91b9a05aa511",
   "metadata": {},
   "source": [
    "I started with the normal imports and headers. Headers were important because they stopped me from being blocked and I was able to scrape the website. The url that I used was from where a person can search through all of the park monuments, as this was what my website is going to do in a more clear way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcf7d5d-c98c-4892-b302-63662940afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://www.nycgovparks.org/art-and-antiquities/permanent-art-and-monuments/results?search_text=%20&borough=&Search=Search\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup_doc = BeautifulSoup(response.content, 'html.parser')\n",
    "soup_doc.prettify()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a3e2a1-c4d9-4fbb-8311-52515aea2476",
   "metadata": {},
   "source": [
    "I then had to list through all of the lists that contained the information that I needed. This was at first confusing for me becuase it had two different names, but it started with the same two words, which is what I ended up using. I then listed through each for the first page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48de3cd8-fcd0-434d-b723-e5a54366c3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=soup_doc.find_all(class_='mon_list')\n",
    "len(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909a968c-5aaf-4990-9f92-e2f6dd875f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1\n",
    "for cell in list1:\n",
    "    print(cell)\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770265a6-2945-4e90-8734-64e763c36e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1\n",
    "for cell in list1:\n",
    "    print(cell.text)\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc01fc2a-4174-4b46-bc09-0e5d758b055d",
   "metadata": {},
   "source": [
    "I then added playwright to go through all of the pages because this is what I understood the most to go through multiple pages, which I ended up going back to BeautifulSoup. I tried a bunch of different things to get it to work and some are listed below. The lists are from different assignments that I compared it to. This was to try to understand exactly how to do it with nowledge from the class (also where the comments come from that were there for my personal use as I knew what the assignments consisted of)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d15a790-4cac-4cbb-bb60-021590c6e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.async_api import async_playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf319b3-a1da-438a-a62c-1108c436b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "playwright = await async_playwright().start()\n",
    "browser = await playwright.chromium.launch(headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6930e3-dff4-4e83-9ae5-55e6c5a862cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = await browser.new_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9ddae3-bfad-4515-8203-ae2db0d1a5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "await page.goto(\"https://www.nycgovparks.org/art-and-antiquities/permanent-art-and-monuments/results?search_text=%20&borough=&Search=Search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789e3241-68b0-45dc-87a3-eba492d47d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = await page.content()\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8a9341-2df8-41b2-955e-3062db5ec701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup_doc.prettify()\n",
    "\n",
    "soup_doc = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d613780-a649-4172-a334-a11e2ce900ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=soup_doc.find_all(class_='mon_list')\n",
    "len(list1)\n",
    "\n",
    "for cell in list1:\n",
    "    print(cell)\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43fc8b3-fb7a-4ea1-9f44-b4a27267641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = soup_doc.find_all(class_='mon_list')\n",
    "\n",
    "for row in rows:\n",
    "    print(\"-----\")\n",
    "    print(row.text.strip())\n",
    "    # sc-46f9336f-9 grYYaW\n",
    "    # print out the title\n",
    "    print('title:', row.find('strong').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea16619-abd6-4321-92d5-9138c36a14b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = soup_doc.find_all(class_='mon_list')\n",
    "data_all = []\n",
    "\n",
    "for row in rows:\n",
    "    print(\"-----\")\n",
    "    print(row.text.strip())\n",
    "    \n",
    "\n",
    "    title = row.find('strong').text\n",
    "    print('title:', title)\n",
    "    \n",
    "    \n",
    "    loc = row.find('em').text\n",
    "    print(\"loc:\", loc)\n",
    "    \n",
    "\n",
    "\n",
    "    desc = row.find_all('p')[-1]\n",
    "    if desc:\n",
    "        ending_text = desc.contents[-1].strip() if desc.contents else \"\"\n",
    "        print(ending_text)\n",
    "        print(\"desc:\", ending_text)\n",
    "        print(\"--------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd71bff-6c46-4a14-b37b-cb60c34b4092",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = []\n",
    "\n",
    "for row in rows:\n",
    "    #print(\"-----\")\n",
    "    print(row.text.strip())\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    # sc-46f9336f-9 grYYaW\n",
    "    # print out the title\n",
    "    # select by class of grYYaW\n",
    "    data['title'] = row.find('strong').text\n",
    "    \n",
    "    # rank, authors, appearances, score\n",
    "    data['loc'] = row.find('em').text\n",
    "    \n",
    "    # find the 3rd cell\n",
    "    data['desc'] = row.find_all('p')[-1]\n",
    "    if desc:\n",
    "        ending_text = desc.contents[-1].strip() if desc.contents else \"\"        \n",
    "\n",
    "    print(data)\n",
    "    data_all.append(data)\n",
    "data_all    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b53a9-c60f-423c-ba67-b62ca87ac96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = []\n",
    "\n",
    "for row in rows:\n",
    "    #print(\"-----\")\n",
    "    print(row.text.strip())\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    # sc-46f9336f-9 grYYaW\n",
    "    # print out the title\n",
    "    # select by class of grYYaW\n",
    "    data['title'] = row.find('strong').text\n",
    "    \n",
    "    # rank, authors, appearances, score\n",
    "    data['loc'] = row.find('em').text\n",
    "    \n",
    "    # find the 3rd cell\n",
    "    data['desc'] = row.find_all('br')[-1].next\n",
    "    if desc:\n",
    "        ending_text = desc.contents[-1].strip() if desc.contents else \"\"        \n",
    "\n",
    "    print(data)\n",
    "    data_all.append(data)\n",
    "data_all    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52bf3a-a60b-482c-8243-8d148d82c961",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = []\n",
    "\n",
    "for row in rows:\n",
    "    #print(\"-----\")\n",
    "    print(row.text.strip())\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    # sc-46f9336f-9 grYYaW\n",
    "    # print out the title\n",
    "    # select by class of grYYaW\n",
    "    data['title'] = row.find('strong').text\n",
    "    \n",
    "    # rank, authors, appearances, score\n",
    "    data['loc'] = row.find('em').text\n",
    "    \n",
    "    # find the 3rd cell\n",
    "    data['desc'] = row.find_all('br')[-1].next\n",
    "    if desc:\n",
    "        ending_text = desc.contents[-1].strip() if desc.contents else \"\"        \n",
    "\n",
    "    print(data)\n",
    "    data_all.append(data)\n",
    "data_all    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ace15b2-34de-4d5a-89b2-86f05a1bbbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc205a-eac2-4f6e-81d9-6078a51aa62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['desc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56958882-6764-46b9-a6df-93938682866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "monuments = [data]\n",
    "monuments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f9aeaa-6151-4ef1-9793-ec1c20128b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.json_normalize(data_all)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa79b8ac-5a74-47e3-87b3-bd30cc5a959a",
   "metadata": {},
   "source": [
    "After several days of practice and asking lots of questions both to professors, peers, and online, this is what I ended up with that gave me the scrape of all 81 pages for park monuments in New York City. My main problem was figuring out how to do the requests. I learned how to use parameters and how to use .append successfully and understand it fully compared to during class. I had to do research mostly on how to get it to go from link to link with it changing page number, which was very useful for multiple assignments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a22d8c3-156a-435e-ad0e-45b9eff0e264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.nycgovparks.org/art-and-antiquities/permanent-art-and-monuments/results\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"\n",
    "}\n",
    "data_all = []\n",
    "for page_number in range(1, 82):\n",
    "    print(f\"Scraping page {page_number}...\")\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params={\"search_text\": \" \", \"borough\": \"\", \"page\": page_number})\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to scrape page {page_number}.\")\n",
    "            continue\n",
    "        soup_doc = BeautifulSoup(response.content, \"html.parser\")\n",
    "        rows = soup_doc.find_all(class_='mon_list')\n",
    "        for row in rows:\n",
    "            data = {}\n",
    "            title_element = row.find('strong')\n",
    "            data['title'] = title_element.text.strip() if title_element else \"No title\"\n",
    "            location_element = row.find('em')\n",
    "            data['loc'] = location_element.text.strip() if location_element else \"No location\"\n",
    "            desc_elements = row.find_all('br')\n",
    "            if desc_elements:\n",
    "                last_text = desc_elements[-1].next\n",
    "                data['desc'] = last_text.strip() if last_text else \"No description\"\n",
    "            else:\n",
    "                data['desc'] = \"No description\"\n",
    "            data_all.append(data)\n",
    "        time.sleep(2)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred\")\n",
    "        continue\n",
    "for data in data_all:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b382b0a5-cd22-42f7-8c7e-32df8f59059f",
   "metadata": {},
   "source": [
    "Then came the last part, exporting into a csv! I simply made it into a file that I could use Pandas on and exported it like we did in class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1424b1-7785-4afb-907b-29ffbee5e4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.json_normalize(data_all)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb29f2-97a3-4cfb-92b9-5c3d6a1ffa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62132f62-415e-4ce8-8c2d-83d8f7a4f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d5ae2-39ef-4ed9-b35c-5dac7d972290",
   "metadata": {},
   "source": [
    "From the scraping, I added to the python file and to the HTML. I uploaded the database so it could pull from it to be able to access all of the parks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff56e6c3-c724-437a-9b6e-485703f6c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from flask import render_template\n",
    "import pandas as pd\n",
    "from flask import Flask, render_template, request\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def hello():\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "@app.route(\"/parks\")\n",
    "def parks():\n",
    "    df = pd.read_csv(\"data.csv\")\n",
    "    parks = df.to_dict('records') \n",
    "    \n",
    "    return render_template(\n",
    "        'index.html',\n",
    "        parks=parks) \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "#This was from my python code to get the basic format of the first website page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570e5554-aeec-4926-ab1d-960ce65dc140",
   "metadata": {},
   "outputs": [],
   "source": [
    "<!doctype html>\n",
    "<html>\n",
    " \n",
    "<style>\n",
    "\n",
    "</style>\n",
    "  <head>\n",
    "    <title>Park Monuments</title><style>\n",
    "      h1 {color:darkolivegreen;}\n",
    "      h3 {color:rgb(6, 34, 6);}\n",
    "      p {color:rgb(14, 68, 27);}\n",
    "      </style>\n",
    "      <div class=\"topnav\">\n",
    "        <a class=\"active\" href=\"#home\">Home</a>\n",
    "        <a href=\"#borough\">Parks by Borough</a>\n",
    "        <input type=\"text\" placeholder=\"Search\">\n",
    "  \n",
    "  </head>\n",
    "  <body style=\"background-color:rgb(212, 219, 218);\">\n",
    "    <h1 style=\"text-align: center;\">Park Monuments in New York City</h1>\n",
    "    <h3 style=\"text-align: center;\">There are {{parks | length}} park monuments in the five boroughs of New York City.</h2>\n",
    "    \n",
    "\n",
    "    <p>\n",
    "      <strong>List of all park monuments:</strong>\n",
    "      <ul>\n",
    "        {% for park in parks %}\n",
    "        <li>\n",
    "            <a href=\"/parks/{{ park['park_code'] }}\">{{ park['title'] }}</a> \n",
    "        </li>\n",
    "        {% endfor %}\n",
    "    </ul>\n",
    "    \n",
    "  </p>\n",
    "\n",
    "  </body>\n",
    "</html>\n",
    "\n",
    "#This is from my html page where I designed the page. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92c7c3f-d4ab-4902-afc5-7c4bce68fc42",
   "metadata": {},
   "source": [
    "Next, I am trying to paginate all of the parks at once, but it is not currently working so I am putting it in as a note. The only thing that is not working is moving to their separate pages. The parks already have links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d28ff7-8b97-4af8-94dc-be8080f0785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "\n",
    "\n",
    "    PER_PAGE = 15\n",
    "    current_page = int(request.args.get('page', 1))\n",
    "    start = PER_PAGE * current_page - PER_PAGE\n",
    "    end = PER_PAGE * current_page\n",
    "    parks = parks[start:end]\n",
    "    pagination = Pagination(total=df.shape[0],\n",
    "                            page=current_page,\n",
    "                            per_page=PER_PAGE)\n",
    "\n",
    "    return render_template(\n",
    "        'index.html',\n",
    "        parks=parks\n",
    "        parks=parks,\n",
    "        pagination=Pagination\n",
    "    )\n",
    "\n",
    "@app.route(\"/parks/<int:parks_id>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
